{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to NatureForm","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"Frontend/FloorplanConfigurator/","title":"Floorplan Editor","text":""},{"location":"Frontend/FloorplanConfigurator/#overview","title":"Overview","text":"<p>The Floorplan feature is an interactive, two-part tool that allows users to create a custom 2D room layout. It's designed to be simple and intuitive, starting users with a basic template and then providing a powerful editor to refine their shape.</p> <p>The entire system is built as a fully vector-based editor, ensuring that shapes can be scaled and manipulated with perfect precision, free from any pixelation.</p>"},{"location":"Frontend/FloorplanConfigurator/#user-workflow","title":"User Workflow","text":"<p>The user experience is broken into two distinct stages:</p> <ol> <li> <p>1. Shape Selection: The user is first presented with a grid of common room \"primitives\" or templates, such as a simple square, a rectangle, or an L-shaped room.</p> </li> <li> <p>2. Interactive Editing: After selecting a template, the user is taken into the Floorplan Workspace. This is an infinite canvas where they can pan, zoom, and directly manipulate the shape to match their exact specifications.</p> </li> </ol> <p>Once finished, the user can save their design, which finalizes the shape's vertex data.</p>"},{"location":"Frontend/FloorplanConfigurator/#key-features-capabilities","title":"Key Features &amp; Capabilities","text":"<p>The workspace provides a rich set of editing tools:</p> <ul> <li> <p>View Navigation:</p> <ul> <li>Pan: Users can click and drag the grid background to pan the viewport.</li> <li>Zoom: Using the mouse wheel, users can zoom in and out, with the zoom centered on the cursor for intuitive control.</li> </ul> </li> <li> <p>Shape Manipulation:</p> <ul> <li>Move Vertices: Users can grab any corner (vertex) of the shape and drag it.</li> <li>Move Edges: Users can grab any wall (edge) and move it, which repositions the two connected vertices along that wall's normal.</li> <li>Add Vertices: By clicking and dragging a special handle at the midpoint of any wall, users can \"pull out\" a new corner, splitting one wall into two.</li> </ul> </li> <li> <p>Precision and Alignment:</p> <ul> <li>Precise Length Input: The length of each wall is displayed as a text label. Users can click this label to type in an exact numerical length for that wall.</li> <li>Grid Snapping: All drag operations (for vertices, edges, or new points) automatically snap to a background grid, ensuring clean lines and easy alignment.</li> </ul> </li> </ul>"},{"location":"Frontend/FloorplanConfigurator/#technical-implementation-highlights","title":"Technical Implementation Highlights","text":"<p>Instead of being a pixel-based (raster) editor like MS Paint, the floorplan tool is a vector graphics editor built using SVG (Scalable Vector Graphics).</p> <ul> <li> <p>Vector-Based by Design: The room's shape is not stored as pixels. It's stored in React state as a simple array of vertices (e.g., <code>[{x: 10, y: 10}, {x: 100, y: 10}, ...]</code>). The SVG <code>&lt;polygon&gt;</code> element renders this data visually.</p> </li> <li> <p>Benefits of SVG:</p> <ul> <li>Infinite Scalability: The shape, grid, and handles can be zoomed in on indefinitely without any loss of quality or pixelation.</li> <li>Object-Based Interaction: Every part of the shape (the main polygon, the circular vertex handles, the edge midpoint handles) is its own SVG element. This makes it simple to attach specific event listeners (like <code>onMouseDown</code>) to each part.</li> </ul> </li> <li> <p>State-Driven Rendering: All interactions are handled by React. When a user drags a vertex, they are simply updating the <code>points</code> array in the component's state. React then re-renders the SVG with the new vertex coordinates, making the editing feel instantaneous.</p> </li> <li> <p>Coordinate Space Transformation: A key piece of the logic is the set of helper functions that translate coordinates. When a user clicks, the editor must translate that screen-space (pixel) coordinate into the local-space (vector) coordinate of the shape, accounting for the current pan and zoom level. This allows a user's mouse movement to be correctly applied to the shape's data.</p> </li> </ul>"},{"location":"Frontend/Framework/","title":"Framework: Why We Use React","text":"<p>This document provides a high-level overview of why React was chosen as the framework for this application and how its core principles are used to manage our interactive tools.</p>"},{"location":"Frontend/Framework/#why-react","title":"Why React?","text":"<p>React is a JavaScript library for building user interfaces. It's particularly well-suited for our project, which involves managing complex, interactive state for both a 2D editor and a 3D configurator.</p> <ul> <li> <p>Component-Based Architecture: Our entire application is built from small, independent, and reusable \"components.\"</p> <ul> <li>The <code>Floorplan</code> selector is a component.</li> <li>The <code>FloorplanWorkspace</code> editor is another component.</li> <li>The <code>Configurator</code> 3D scene is a component.</li> <li>Even the UI sliders and buttons are individual components.     This makes the code easy to manage, test, and update without one part breaking another.</li> </ul> </li> <li> <p>Declarative UI (State Management): This is the most important concept.</p> <ul> <li>Traditional (Imperative): In older code, you'd write step-by-step instructions like: \"find the slider,\" \"get its value,\" \"find the 3D model,\" \"update the model's X position.\" This is complex and error-prone.</li> <li>React (Declarative): We simply declare the state we want. We have a state variable, <code>modelPositionX</code>. The slider's value is bound to this state, and the 3D model's position is derived from this state. When the user moves the slider, it updates the state, and React automatically ensures the 3D model's position updates to match. We just describe the \"what\" (the model's position should match this variable), and React handles the \"how.\"</li> </ul> </li> <li> <p>Rich Ecosystem: React is the most popular UI library, meaning it has a massive community and integrates well with other libraries. This is crucial for us, as it simplifies the integration of powerful tools like <code>three.js</code> (for 3D graphics) and <code>react-router-dom</code> (for navigation).</p> </li> </ul>"},{"location":"Frontend/Framework/#how-it-all-works-together-data-flow","title":"How It All Works Together: Data Flow","text":"<p>React uses a one-way data flow, which makes the application predictable. Data flows \"down\" from parent components to children, and events (like user clicks) flow \"up\" via callback functions.</p> <p>Here is a simplified diagram of our application's architecture:</p> <pre><code>[ App.js ]  &lt;---------------------------------------+\n   |                                                | 4. `onFinish` callback updates the\n   | (State: floorplanPoints)                       |    `floorplanPoints` state.\n   |                                                |\n   +--&gt; [ Floorplan.js ] --- (passes onFinish) ----&gt; [ FloorplanWorkspace.js ]\n   |      (Renders workspace)                         |\n   |                                                  | 3. User clicks \"Finish,\"\n   |                                                  |    and workspace calls `onFinish(points)`.\n   |                                                  |\n   +--&gt; [ Configurator.js ] &lt;------------------------+\n        (Receives floorplanPoints as a \"prop\")        |\n                                                      | 5. React sees state changed\n                                                      |    and re-renders Configurator\n                                                      |    with the *new* points.\n</code></pre>"},{"location":"Frontend/Framework/#the-step-by-step-flow","title":"The Step-by-Step Flow","text":"<ol> <li> <p>Top-Level State: The main <code>App</code> component holds the most important piece of state: <code>floorplanPoints</code>.</p> </li> <li> <p>Data Down (Props):</p> <ul> <li>The <code>App</code> component renders the <code>Floorplan</code> component, passing it a function prop called <code>onFinish</code>.</li> <li>The <code>App</code> component also renders the <code>Configurator</code> component, passing it the current <code>floorplanPoints</code> array as a prop.</li> </ul> </li> <li> <p>Events Up (Callbacks):</p> <ul> <li>When the user finishes drawing in the <code>FloorplanWorkspace</code>, they click the \"Finish\" button.</li> <li>The workspace calls the <code>onFinish</code> function it received, passing the new array of points (e.g., <code>onFinish([...])</code>).</li> </ul> </li> <li> <p>State Update:</p> <ul> <li>This <code>onFinish</code> function lives in the main <code>App</code> component. When called, it updates the <code>floorplanPoints</code> state inside <code>App</code> with the new points.</li> </ul> </li> <li> <p>Automatic Re-render:</p> <ul> <li>React detects that the <code>floorplanPoints</code> state has changed.</li> <li>It automatically re-renders any component that depends on that state. In this case, it re-renders the <code>Configurator</code> component.</li> <li>The <code>Configurator</code> now receives the new <code>floorplanPoints</code> as its prop, triggering its internal <code>useEffect</code> hook to discard the old 3D floor and generate a new one.</li> </ul> </li> </ol>"},{"location":"Frontend/Framework/#the-bridge-to-threejs-useeffect","title":"The Bridge to <code>three.js</code>: <code>useEffect</code>","text":"<p>React manages the DOM, but <code>three.js</code> manages a <code>&lt;canvas&gt;</code> element. We use the <code>useEffect</code> hook as the bridge between these two worlds.</p> <p>In the <code>Configurator</code> component, the <code>useEffect</code> hook essentially says: \"When the <code>floorplanPoints</code> prop changes, run this code to build (or re-build) the <code>three.js</code> scene.\"</p> <p>This hook is also responsible for cleanup. When the component is removed, the <code>useEffect</code>'s return function runs, which safely disposes of all <code>three.js</code> geometries, materials, and the renderer. This prevents memory leaks and is a critical part of mixing React with external libraries.</p>"},{"location":"Frontend/SilhouetteExtractor/","title":"Extracting a 2D Silhouette","text":"<p>This document outlines the process for dynamically generating a 2D silhouette polygon from a 3D model. This technique is highly effective for creating accurate 2D physics colliders for 3D objects, especially in a top-down perspective, as it accounts for the model's current shape (e.g., from an animation).</p>"},{"location":"Frontend/SilhouetteExtractor/#the-process","title":"The Process","text":"<p>The workflow involves rendering the object's shape to a texture and then using image processing algorithms to convert that 2D image into a 2D physics-ready polygon.</p>"},{"location":"Frontend/SilhouetteExtractor/#1-optimize-model-optional","title":"1. Optimize Model (Optional)","text":"<p>To improve the performance of the render-to-texture step, it's beneficial to use a lower-polygon version of your model. This step can often be skipped if the source model is already reasonably low-poly.</p>"},{"location":"Frontend/SilhouetteExtractor/#2-setup-top-down-orthographic-camera","title":"2. Setup Top-Down Orthographic Camera","text":"<p>Position an orthographic camera directly above the model, looking straight down. This camera will be used to capture the 2D \"shadow\" or outline of the object on the X/Z plane.</p>"},{"location":"Frontend/SilhouetteExtractor/#3-render-to-texture","title":"3. Render-to-Texture","text":"<p>Render the 3D object (using its dynamic, possibly animated, shape) into a separate, low-resolution off-screen buffer, also known as a render target.</p> <p>This render must use a simple, unlit shader that renders the object as pure white against a pure black background. The resulting texture is a binary (black and white) 2D representation of the model's silhouette.</p>"},{"location":"Frontend/SilhouetteExtractor/#4-read-pixels","title":"4. Read Pixels","text":"<p>Read the pixel data from the render target texture back to the CPU. This gives you a 2D grid of binary data (1s for white, 0s for black) that represents the object's shape.</p>"},{"location":"Frontend/SilhouetteExtractor/#5-trace-outline","title":"5. Trace Outline","text":"<p>Process this 2D pixel grid using an outline-tracing algorithm like Marching Squares. This algorithm \"walks\" the boundary between the white (object) and black (empty) pixels, generating a 2D polygon (a list of 2D vertices) that describes the outline of the \"blob.\"</p>"},{"location":"Frontend/SilhouetteExtractor/#6-simplify-polygon-optional","title":"6. Simplify Polygon (Optional)","text":"<p>The polygon generated by Marching Squares will be very detailed and \"jagged,\" as it follows the pixel edges perfectly. This high vertex count can be inefficient for a physics engine.</p> <p>If needed, use a polygon simplification algorithm (such as Ramer-Douglas-Peucker) to reduce the vertex count significantly while preserving the overall shape. This step may not be necessary if the render-to-texture resolution is very low, as the resulting polygon will already be quite simple.</p>"},{"location":"Frontend/SilhouetteExtractor/#7-decompose-polygon-if-needed","title":"7. Decompose Polygon (If Needed)","text":"<p>Most 2D physics engines require collision shapes to be convex (i.e., no \"dents\" or \"caves\"). If the silhouette shape is concave (e.g., a \"U\" shape), it must be broken down into a set of smaller, convex polygons. This is achieved by running the polygon's vertices through a 2D convex decomposition algorithm.</p>"},{"location":"Frontend/SilhouetteExtractor/#8-normalize-coordinates","title":"8. Normalize Coordinates","text":"<p>The final step is to convert the polygon's vertices from their pixel coordinates (e.g., 0-256) back into the 2D world-space or local-space coordinates that the physics engine understands (e.g., mapping the pixel positions back to the model's X and Z coordinates).</p>"},{"location":"Frontend/SilhouetteExtractor/#final-output","title":"Final Output","text":"<p>The result of this process is a set of one or more 2D convex polygons that accurately represent the top-down silhouette of the 3D model, ready to be used by a 2D physics engine.</p>"},{"location":"Frontend/ThreeJS/","title":"Framework: Why We Use Three.js","text":"<p>This document covers why Three.js was chosen as our 3D rendering library and the basic concepts of how it powers our <code>Configurator</code> component.</p>"},{"location":"Frontend/ThreeJS/#why-threejs","title":"Why Three.js?","text":"<p>Three.js is a 3D graphics library that renders complex 3D scenes in a web browser using WebGL. It's the engine that draws everything inside our <code>Configurator</code>'s <code>&lt;canvas&gt;</code> element.</p> <ul> <li> <p>It's an Abstraction, Not a Game Engine:     We don't need a heavy, all-in-one platform like Unity or Unreal Engine. We just need a powerful library that can draw 3D objects inside our existing React application. Three.js is the perfect fit. It gives us low-level control without forcing us to write raw, complex WebGL code.</p> </li> <li> <p>It's Just JavaScript:     There are no plugins or external software. It's a JavaScript library that integrates directly into our React component. This allows us to pass data (like the <code>floorplanPoints</code>) from our 2D React components straight into the 3D scene.</p> </li> <li> <p>Performance:     It's a very thin layer over WebGL, which means it runs directly on the computer's GPU. This allows us to render complex models, lighting, and shadows at a high frame rate (60fps).</p> </li> <li> <p>Vast Ecosystem &amp; Community:     Three.js is the most popular 3D library for the web. This means it has an enormous community and a vast collection of \"helpers\" and utilities. We use several of these:</p> <ul> <li><code>GLTFLoader</code>: To load our 3D sofa model.</li> <li><code>OrbitControls</code>: To provide the \"click and drag\" camera controls out of the box.</li> <li><code>BufferGeometryUtils</code>: To merge our dynamic \"footprint\" geometry into a single, efficient mesh.</li> </ul> </li> </ul>"},{"location":"Frontend/ThreeJS/#how-it-works-the-core-concepts","title":"How It Works: The Core Concepts","text":"<p>Think of a Three.js application as a movie set. To film anything, you need three basic things:</p> <ol> <li> <p>A <code>Scene</code>: This is the stage or the virtual world. It's an empty container where you place all your objects, models, and lights.</p> </li> <li> <p>A <code>Camera</code>: This is the \"eye\" that looks at the scene. We use a <code>PerspectiveCamera</code>, which mimics a real-world camera (objects farther away appear smaller).</p> <ul> <li>(Note: Our <code>SilhouetteExtractor</code> utility uses an <code>OrthographicCamera</code>, which is a \"flat\" camera with no perspective, like a 2D blueprint.)</li> </ul> </li> <li> <p>A <code>Renderer</code>: This is the \"artist\" and crew. It takes the <code>Scene</code> and the <code>Camera</code>, calculates what the scene looks like from the camera's point of view, and draws the final 2D image onto the HTML <code>&lt;canvas&gt;</code> element. This \"drawing\" happens in a loop (using <code>requestAnimationFrame</code>) 60 times per second.</p> </li> </ol>"},{"location":"Frontend/ThreeJS/#whats-in-our-scene","title":"What's in Our Scene?","text":"<p>Our scene is filled with a few key types of objects:</p> <ul> <li> <p>Meshes: A <code>Mesh</code> is the main 3D object you see. Every <code>Mesh</code> is a combination of two things:</p> <ol> <li>Geometry: The shape or 3D data (the vertices and faces).<ul> <li>This can be a primitive (like <code>BoxGeometry</code>).</li> <li>It can be a loaded model (like our sofa).</li> <li>It can be custom-generated (like our <code>ShapeGeometry</code>, which we create from the user's <code>floorplanPoints</code>!).</li> </ul> </li> <li>Material: The skin or surface (the color, texture, and shininess).<ul> <li>We use <code>MeshStandardMaterial</code> for the floor and sofa because it's a realistic material that reacts to light.</li> <li>We use <code>MeshBasicMaterial</code> for the green \"footprint,\" as it's a simple, flat color that is not affected by lights.</li> </ul> </li> </ol> </li> <li> <p>Lights: Without lights, most materials would appear black.</p> <ul> <li><code>AmbientLight</code>: Provides a soft, general light to the whole scene so nothing is in pure black shadow.</li> <li><code>DirectionalLight</code>: Acts like the \"sun.\" It shines from one direction and is the source that allows our sofa to cast realistic shadows onto the floor.</li> </ul> </li> </ul>"},{"location":"Frontend/ThreeJS/#how-it-fits-in-our-react-app","title":"How It Fits in Our React App","text":"<p>We don't use Three.js by itself; we wrap it entirely within our <code>Configurator</code> React component.</p> <ul> <li> <p>The <code>useEffect</code> Hook is the Bridge:     Our <code>Configurator</code> has a <code>useEffect</code> hook that runs when the component first appears. This hook is where we call our <code>init()</code> function to:</p> <ol> <li>Create the <code>Scene</code>, <code>Camera</code>, and <code>Renderer</code>.</li> <li>Create the <code>&lt;canvas&gt;</code> element and append it to the DOM.</li> <li>Load the 3D model, create the lights, and start the <code>animate</code> loop.</li> </ol> </li> <li> <p>State &amp; Props as Triggers:     The <code>useEffect</code> hook is set to \"watch\" the <code>floorplanPoints</code> prop. If the user goes back and changes their floorplan, the <code>floorplanPoints</code> prop changes, and React re-runs the hook. This triggers our logic to throw away the old 3D floor and generate a new one from the new points.</p> </li> <li> <p>Cleanup is Essential:     The <code>return</code> function inside the <code>useEffect</code> hook is our cleanup crew. When the user navigates away from the <code>Configurator</code>, this function runs and properly disposes of all the Three.js objects (geometries, materials, the renderer). This is critical for preventing memory leaks in a React application.</p> </li> </ul>"}]}